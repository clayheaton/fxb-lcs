{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Syrian War Timeline off of Wikipedia\n",
    "\n",
    "Wikipedia has one of the richest timelines of the Syrian Civil War that includes references to news articles and others sources for information verification. The timeline, however, is not structured in a manner that makes it easy to export into an Excel sheet or other analysis package.\n",
    "\n",
    "At the heart of this problem is the fact that there is a many-to-many relationship between the references on each page and the statements that cite them. This script uses the BeautifulSoup library and Python 3.6 to scrape the data from the Wikipedia pages and parse it into a SQLite database with a many-to-many structure, allowing for flexible querying of the data and exporting into a format suitable for analysis in an external application.\n",
    "\n",
    "This script was created by Clay Heaton, for Harvard's FXB Center for Health and Human Rights, as part of The Lancet's Commission on Syria in July of 2017. More information can be found [at this link](https://epicenter.wcfia.harvard.edu/blog/documenting-burden-war-syrians).\n",
    "\n",
    "To run this script, you need to running Python 3.5 or greater and have the following libraries installed and accessible to your Jupyter Notebook.\n",
    "\n",
    "- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [arrow](http://arrow.readthedocs.io/en/latest/)\n",
    "- [dateparser](https://dateparser.readthedocs.io/en/latest/)\n",
    "- [dataset](http://dataset.readthedocs.io/en/latest/)\n",
    "\n",
    "Note that variations between the pages scraped make it impractical to effectively pull out the dates from all of the timeline statements. Several passes are made to extract the date but ultimately, research assistants involved with the project will review each citation and timeline statement for veracity and accuracy, correcting malformed dates in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import dataset\n",
    "import requests\n",
    "import arrow\n",
    "import hashlib\n",
    "import dateparser\n",
    "\n",
    "db = dataset.connect(\"sqlite:///syria_timeline.sqlite\")\n",
    "\n",
    "months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\n",
    "          \"September\",\"October\",\"November\",\"December\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    db['pages'].drop()\n",
    "    db['citations'].drop()\n",
    "    db['statements'].drop()\n",
    "    db['citations_statements'].drop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tab_pages = db['pages']\n",
    "tab_citations = db['citations']\n",
    "tab_statements = db['statements']\n",
    "tab_join = db['citations_statements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pages = [\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(January–April_2011)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(May–August_2011)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(September–December_2011)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(January–April_2012)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(May–August_2012)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(September–December_2012)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(January–April_2013)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(May–December_2013)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(January–July_2014)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(August–December_2014)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(January–July_2015)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(August–December_2015)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(January–April_2016)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(May–August_2016)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(September–December_2016)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(January–April_2017)\",\n",
    "    \"https://en.wikipedia.org/wiki/Timeline_of_the_Syrian_Civil_War_(May_2017–present)\"\n",
    "]\n",
    "\n",
    "for page in pages:\n",
    "    rec = {\"url\":page,\"year\":page[-5:-1]}\n",
    "    if rec['year'] == 'sent':\n",
    "        rec['year'] = 2017\n",
    "    rec_id = tab_pages.insert(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def citation_record_from_bottom(file_id,soup_element):\n",
    "    \"\"\"\n",
    "    Send in a BeautifulSoup li tag from the References and return a\n",
    "    dict that can be inserted into the references table in the local db.\n",
    "    \"\"\"\n",
    "    reference_record = {}\n",
    "\n",
    "    # The id on the reference links the citations to the reference\n",
    "    f_id = soup_element.get('id')\n",
    "    reference_record['citation_anchor'] = f_id\n",
    "\n",
    "    # External link in citation\n",
    "    try:\n",
    "        f_ex_link = soup_element.find(class_='external').get('href')\n",
    "        reference_record['external_link'] = f_ex_link\n",
    "    except:\n",
    "        reference_record['external_link'] = None\n",
    "\n",
    "    # Reference Text as a larger chunk\n",
    "    try:\n",
    "        f_ref_text = soup_element.find('span',class_='reference-text')\n",
    "        reference_record['citation_text'] = f_ref_text.text.replace('\\xa0','')\n",
    "    except:\n",
    "        reference_record['citation_text'] = None\n",
    "    \n",
    "    reference_record['page_id'] = file_id\n",
    "    return reference_record\n",
    "\n",
    "def get_reference_link_from_citation(citation):\n",
    "    try:\n",
    "        return list(citation.children)[0].get('href')[1:]\n",
    "    except:\n",
    "        print(\"Error getting citation link\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_dd_date(elem):\n",
    "    if elem.parent.name == 'dd':\n",
    "        try:\n",
    "            while elem.name != 'dl':\n",
    "                elem = elem.parent\n",
    "                if elem.name == 'dl':\n",
    "                    for e in elem.previous_siblings:\n",
    "                        if e and e.name == 'ul':\n",
    "                            try:\n",
    "                                date = list(list(e.children)[1].children)[0].text\n",
    "                                break\n",
    "                            except:\n",
    "                                date = list(list(e.children)[1].children)[0]\n",
    "                                break\n",
    "                    break\n",
    "                    \n",
    "            if any(month in date for month in months):\n",
    "                return date\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_nearest_date(elem, counter=0):\n",
    "\n",
    "    date = None\n",
    "    \n",
    "    # dd elements often have the date as the first child\n",
    "    # of the previous sibling to their parent element\n",
    "    if elem.parent.name == 'dd':\n",
    "        date = extract_dd_date(elem)\n",
    "        if date and any(month in date for month in months):\n",
    "            return date\n",
    "    \n",
    "    try:\n",
    "        date = list(r.parent.children)[0].text\n",
    "    except:\n",
    "        try:\n",
    "            date = list(list(r.parent.parent.previous_sibling.previous_sibling.children)[1].children)[0].text\n",
    "        except:\n",
    "            date = None\n",
    "    \n",
    "    if date and any(month in date for month in months):\n",
    "        return date\n",
    "\n",
    "    if any(month in elem for month in months):\n",
    "        return elem.text\n",
    "    \n",
    "    for child in elem.children:\n",
    "        if any(month in child for month in months):\n",
    "            return child\n",
    "    \n",
    "    for sibling in elem.previous_siblings:\n",
    "        if any(month in sibling for month in months):\n",
    "            return sibling\n",
    "\n",
    "        try:\n",
    "            for child in sibling.children:\n",
    "                if any(month in child for month in months):\n",
    "                    return child\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # We haven't found anything if we are here\n",
    "    elem = elem.parent\n",
    "    if elem is not None:\n",
    "        find_nearest_date(elem,counter+1)\n",
    "\n",
    "def extract_li_date(ref):\n",
    "    try:\n",
    "        ref = ref.parent.parent.previous_sibling.previous_sibling\n",
    "        t = list(ref.children)[0].text.strip()\n",
    "        return t\n",
    "    except:\n",
    "        return None        \n",
    "\n",
    "def extract_h3_date(ref,element_type='h3'):\n",
    "    ref = ref.parent\n",
    "    try:\n",
    "        while ref.name != element_type:\n",
    "            ref = ref.previous_sibling\n",
    "            if ref == None:\n",
    "                ref = ref.previous_sibling\n",
    "\n",
    "        ref = ref.find(class_='mw-headline')\n",
    "        t = ref.text\n",
    "        if '–' in t:\n",
    "            return t.split('–')[0].strip()\n",
    "        elif '-' in t:\n",
    "            return t.split('-')[0].strip()\n",
    "        else:\n",
    "            return t\n",
    "            \n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "def extract_h4_date(ref,element_type='h4'):\n",
    "    ref = ref.parent\n",
    "    try:\n",
    "        while ref.name != element_type:\n",
    "            ref = ref.previous_sibling\n",
    "            if ref == None:\n",
    "                ref = ref.previous_sibling\n",
    "\n",
    "        ref = ref.find(class_='mw-headline')\n",
    "        t = ref.text\n",
    "        if '–' in t:\n",
    "            return t.split('–')[0].strip()\n",
    "        elif '-' in t:\n",
    "            return t.split('-')[0].strip()\n",
    "        else:\n",
    "            return t\n",
    "            \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_timeline_element(ref,year,use_alt_date=False):\n",
    "    row = {}\n",
    "    row['statement'] = ref.parent.text\n",
    "    row['year'] = year\n",
    "    try:\n",
    "        if use_alt_date is True:\n",
    "            d = extract_h4_date(ref)\n",
    "            \n",
    "            if d is None:\n",
    "                d = extract_h3_date(ref)\n",
    "                \n",
    "            if d is None:\n",
    "                d = extract_li_date(ref)\n",
    "        else:\n",
    "            d = find_nearest_date(ref)\n",
    "        \n",
    "        row['date_guess'] = d + \" \" + row['year']\n",
    "    except:\n",
    "        row['date_guess'] = None\n",
    "    row['date_guess_formatted'] = \"\"\n",
    "    row['ref_number_in_statement'] = ref.text\n",
    "    try:\n",
    "        row['date_guess_formatted'] = arrow.get(dateparser.parse(row['date_guess'],date_formats=['%d %B %Y'])).format(\"YYYY-MM-DD\")\n",
    "        if arrow.get(row['date_guess_formatted']).date().year > int(row['year']):\n",
    "            row['date_guess_formatted'] = None\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Hash the content for de-duplication\n",
    "    h = hashlib.sha256()\n",
    "    h.update(row['statement'].encode())\n",
    "    row['hash'] = h.hexdigest()\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_page(ref_page_record):\n",
    "    k = ref_page_record['id']\n",
    "    v = ref_page_record['url']\n",
    "    page_year = ref_page_record['year']\n",
    "\n",
    "    response = requests.get(v)\n",
    "    content = response.content\n",
    "    soup = BeautifulSoup(content,'html.parser')\n",
    "    \n",
    "    # Isolate the citations at the bottom of the page\n",
    "    ref_ol = soup.find(class_=\"references\")\n",
    "    references = []\n",
    "\n",
    "    # Isolate each individual element in the citations\n",
    "    # and create a database entry\n",
    "    for elem in ref_ol.find_all('li'):\n",
    "        references.append(citation_record_from_bottom(k,elem))\n",
    "        \n",
    "    # Try inserting them into the database\n",
    "    tab_citations.insert_many(references)\n",
    "    \n",
    "    # Insert the statements\n",
    "    citations = soup.find_all('sup',class_=\"reference\")\n",
    "    \n",
    "    # Used to deduplicate\n",
    "    page_citations = {}\n",
    "    \n",
    "    for citation in citations:\n",
    "        join_link = get_reference_link_from_citation(citation)\n",
    "        if k == 1:\n",
    "            statement = build_timeline_element(citation,page_year)\n",
    "        else:\n",
    "            statement = build_timeline_element(citation,page_year,True)\n",
    "\n",
    "        h = statement['hash']\n",
    "        \n",
    "        try:\n",
    "            statement_link = v + \"#\" + citation.get('id')\n",
    "        except:\n",
    "            statement_link = None\n",
    "            \n",
    "        statement['statement_url'] = statement_link\n",
    "        \n",
    "        if h not in page_citations.keys():\n",
    "            page_citations[h] = {}\n",
    "            statement_ref_number = statement['ref_number_in_statement']\n",
    "            del statement['ref_number_in_statement']\n",
    "            del statement['hash']\n",
    "            \n",
    "            statement_id = tab_statements.insert(statement)\n",
    "            page_citations[h]['id'] = statement_id\n",
    "            page_citations[h]['links'] = [(k,join_link,statement_ref_number, statement_id)]\n",
    "            page_citations[h]['statement'] = statement\n",
    "        else:\n",
    "            page_citations[h]['links'].append((k,join_link,statement['ref_number_in_statement'],page_citations[h]['id']))\n",
    "            \n",
    "        \n",
    "    # At this point, all of the statements should be in the database.\n",
    "    # Insert the links into the citations_statments table\n",
    "    # collect them and make records\n",
    "    join_records = []\n",
    "    for h in page_citations.keys():\n",
    "        links = page_citations[h]['links'] # list of tuples\n",
    "        for l in links:\n",
    "            join_rec = {\"page_id\":l[0],\"citation_anchor\":l[1],\"text_ref_number\":l[2],\"statement_id\":l[3]}\n",
    "            reference = tab_citations.find_one(page_id=join_rec['page_id'],\n",
    "                                                citation_anchor=join_rec['citation_anchor'])\n",
    "            if reference is not None:\n",
    "                join_rec['citation_id'] = reference['id']\n",
    "                del join_rec['page_id']\n",
    "                del join_rec['citation_anchor']\n",
    "                \n",
    "                join_records.append(join_rec)\n",
    "            else:\n",
    "                # Odd footnote statements may not have an entry in the citations table\n",
    "                continue\n",
    "\n",
    "    tab_join.insert_many(join_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records = tab_pages.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for r in records:\n",
    "    process_page(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
